{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer in Text: Exploration and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors cite lack of parallel corpora and reliable evaluation metrics as the roadblocks for style transfer in natural language processing.\n",
    "\n",
    "They aim to learn separate content representations and style representations, as is the case with pretty much any work dealing with style transfer in computer vision or natural language processing.\n",
    "\n",
    "They measure 2 aspects of style transfer, namely transfer strength and content preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The base method compared against is an autoencoder framework\n",
    "* The authors employ 2 models:\n",
    "    1. Multi-decoder [seq2seq model](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) that use the different decoders for different styles.\n",
    "    2. Style-embeddings augmented decoder (single decoder) to generate outputs in different styles.\n",
    "* Similar to the [persona-based neural conversation model](https://arxiv.org/abs/1603.06155), a style embedding is learned for each different style. The conditional generation is done using recurrent neural networks with the inputs being the recurrent networks current state, and the style embedding to apply.\n",
    "* The methods are evaluated in the following manner:\n",
    "    * Transfer strength is evaluated using a simple classifier\n",
    "    * Content preservation is evaluated by computing the cosine distance between the original and the generated text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The authors don't explain:\n",
    "    * Why is a vanilla autoencoder the base model being compared with? It's objective does not optimize for transferring style.\n",
    "    *  How are the outputs of multiple decoders in model 1 combined into a single output, because the loss being considered is the combined sequence loss of all the decoder outputs.\n",
    "    * What is considered representative embeddings for the input and generated sequence for model 2?\n",
    "    * What ratings qualify as a positive/negative review?\n",
    "* The results indicate the the models proposed by the author in general perform better than the auto-encoder for the purposes of transfer strength."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv",
   "language": "python",
   "name": ".pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
