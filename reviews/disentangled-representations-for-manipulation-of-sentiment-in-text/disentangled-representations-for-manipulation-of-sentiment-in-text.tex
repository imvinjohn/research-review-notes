\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}


\hypersetup{colorlinks=true, citecolor=blue}

\begin{document}

\title{Disentangled Representations for Manipulation of Sentiment in Text}
\author{}
\date{}
\maketitle

\section{Idea}
  The main idea of the paper is to change the style (sentiment) of a body of text while retaining its content

\section{Background}
  Similar to the Persona-Based Neural Conversation Model \cite{li2016persona}, this paper also utilizes an embedding for a particular sentiment that the decoder is conditioned on.

\section{Method}
  \begin{itemize}
    \item This system uses a CNN for text encoding
    \item In the case of sentiment, 2 distinct probability distributions $P_{source}$ and $P_{target}$ are learned. The style transfer is achieved by traversing the manifold between these 2 distributions.
    \item The initial training phase just uses a variational autoencoder-like setup to recreate the original sentences.
    \item After the initial training, the CNN is trained to classify sentiment, thus, causing the distribution of the encoded sentences to diverge based on whether the sentence is positive or negative.
    \item Decoding is done by conditioning the start of the sentence on a start-of-sentence token and the sentence's encoding. The sentence generation ends when an EOS (end-of-sentence) token is generated.
  \end{itemize}

\section{Observations}
  It is not clear how the representations are disentangled. It seems like the sentence encoding itself encodes information about the sentiment and hence, the representations rely on the entanglement to generate the manifold which is traversed.

\bibliographystyle{unsrt}
\bibliography{disentangled-representations-for-manipulation-of-sentiment-in-text}

\end{document}
