\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}


\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\begin{document}

\title{Beam Search Strategies for Neural Machine Translation}
\author{}
\date{}
\maketitle

\section{Main Idea}
  The standard beam search strategy for Neural Machine Translation (NMT) is for the decoder to predict the target sequence word-by-word and maintain a fixed amount of potential word candidates to predict at each step. 
  
  The drawbacks of this approach are that it is less adaptive, because: 
  \begin{itemize}
    \item Some candidates might not be as good as the current best
    \item Good candidates might not be considered because they missed out on the threshold for candidate inclusion marginally. Addressing this by naively increasing beam search size will result in slowing down the decoder.
    $$beam\_size \propto model\_accuracy$$
    $$beam\_size \propto \frac{1}{model\_performance}$$
  \end{itemize}

  The paper proposes a more flexible decoder strategy, by pruning the search graph, and reducing the number of candidates with the same partial hypothesis (shared past).

\section{Background}
  Standard beam search builds a translation from left-to-right and keeps a fixed
  number (beam) of translation candidates with the highest log-probability at each time step. If an end-of-sequence (EOS) token is encountered, we reduce the beam size by 1 and add the sequence to the candidate list of sequences. When the beam size eventually becomes 0, we evaluate the log-probability of all the sequences weighted by sequence length and pick the translation with the highest log-probability score.

\section{Method}
  4 separate beam search strategies are proposed. One or more of these can be applied on top of vanilla beam search.
  \begin{itemize}
    \item Relative threshold pruning
  \end{itemize}

\section{Observations}
  \begin{itemize}
    \item 
  \end{itemize}

\bibliographystyle{unsrt}
\bibliography{beam-search-strategies-for-neural-machine-translation}

\end{document}
