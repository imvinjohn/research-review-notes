\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}


\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\begin{document}

\title{Adversarial Learning for Neural Dialogue Generation}
\author{}
\date{}
\maketitle

\section{Main Idea}
  The authors formulate this dialogue model as a reinforcement learning problem. The network used is a Generative Adversarial Network. The discrimnator object is the same as a Turing test predictor i.e. classifies whether the dialogue response is human or machine-generated. The goal is to improve to improve the generator to the point where the discrimnator has trouble distinguishing between human and machine-generated responses. \cite{li2017adversarial}.

\section{Method}
  \begin{itemize}
    \item The generator network is a neural seq2seq model, and the discrimnator is similar to a Turing test evaluator
    \item The generation task is not formulated as a NMT task. Instead, it tries to maximize the likelihood of a response $y = \{y_1, y_2 ... y_T\}$ given a history of previous sentences $x$.
    \item The generator defines the policy by which each word of the output sentence $y$ is generated using a softmax over the space of the vocabulary.
    \item The discrimnator uses a hierarchical neural autoencoder \cite{li2015hierarchical} to generate a vector representation of an entire sequence of conversation i.e. $\{x, y\}$. This vector representation is then fed into a binary classifier which predicts whether the sentences were human- or machine-generated.
    \item The generator is trained to maximize the expected reward of the generated utterance using the REINFORCE algorithm. \cite{williams1992simple}.
    \item The vanillla REINFORCE model doesn't assign rewards to each generated word, and rather assigns equal reward to all the tokens within a predicted sequence of words.
    \item However, for partially decoded sequences, the discrimnator must also be capable of generating classifications for partial sequences. Two methods are proposed to solve this:
      \begin{itemize}
        \item Using a Monte-Carlo search to decode $N ( = 5)$ top candidate sentences given a partial sequences and using the discrimnator average of the 5 complete sequences to predict the classification for the partial sequence.
        \item Training the discrimnator to directly also be able to classify partial sequences.
      \end{itemize}
    The Monte-Carlo search strategy was found to be more effective.
    \item Teacher forcing is used to essentially short-circuit the distance between the generator and the true sequence.
    \item The generative model is trained using seq2seq \cite{sutskever2014sequence} and an attention mechanism \cite{bahdanau2014neural}.The discrimnator is also pretrained using part of the training data and generating sequences by beam-search and sampling.
    \item Intuitively low accuracy of a reasonably well trained discrimnator would imply that the quality of generated sentences have improved significantly.
  \end{itemize}

\section{Observations}
  \begin{itemize}
    \item The authors report that the responses generated by their system are more interactive, interesting, and
    non-repetitive. It'd be interesting to see how they quantify this. UPDATE: The source for this claim is human evaluations, which of course, could be subjective.
    \item It's also observed that the system yielded better results when the context i.e. the $x$ preceding utterances were limited to 2.
    \item The hierarchical neural model is the architecture of choice for the discrimnator (evaluator).
  \end{itemize}

\bibliographystyle{unsrt}
\bibliography{adversarial-learning-for-neural-dialogue-generation}

\end{document}
