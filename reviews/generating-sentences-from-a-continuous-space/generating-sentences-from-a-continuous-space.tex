\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}


\hypersetup{
    colorlinks=true,
    citecolor=blue
}

\begin{document}

\title{Generating Sentences from a Continuous Space}
\author{}
\date{}
\maketitle

\section{Main Idea}
  The paper presents an alternative strategy to language modeling using RNNs. The authors attempt to use this to impute missing words in a sentence, to interpolate between the latent representations of 2 sentences, and also generate sentences by sampling from the space of the latent representation's prior probability. \cite{bowman2016generating}

\section{Background}
  \begin{itemize}
    \item The authors differentiate between types of sentence embeddings. \textbf{Sequence Autoencoders} have RNNs as encoders and decoders are are just used to regenerate the original text. \textbf{Skip-Thought models} are similar but the target sentence is different from the original sentence. \textbf{Paragraph Vector models} simply predict the words that are present in a given sentence.
    \item Variational autoencoders impose a prior distribution on the latent representation, but a standard autoencoder does not.
    \item The latent representation is isuualy parametrized by a diagonal Gaussian distribution.
  \end{itemize}

\section{Method}
  \begin{itemize}
    \item The prior distribution of the latent representation acts as a regularizer.
    \item Unclear as to what a 'global latent representation' is. Intuitively, each sentence would have its own representation.
    \item The authors suggest KL-term annealing, which involves having a cost function like
    $$L(\theta; x) = \alpha (-KL(q_{\theta}(z|x)||p(z))) + E_{q_{\theta}(z|x)}[\log p_{\theta}(x|z)]$$ where the value of $\alpha$ is raised from 0 to 1 during the course of the training. This can be thought of as a steady progression from a standard autoencoder to a VAE.
    \item Word-level dropout used to force the decoder to rely primarily on the latent space for the sentence generation.
    \item A beam search strategy is used by the decoder with beam-size 15.
    \item Training sequences are read from right to left, to shorten the dependencies.
    \item An alternative to qualitative evaluation is presented by the usage of an adversarial classifier, which is partially similar to what a GAN does. Non-differentiability of the 
    \item Model is compared against RNNLMs. \cite{mikolov2011extensions}
  \end{itemize}

\section{Observations}
  \begin{itemize}
    \item The paper doesn't talk about the presence of dead-zones in the latent space. This should be more of a problem due to the discrete nature of word representation. It also doesn't explain why the reconstruction error doesn't dominate, and only the KL-divergence error dominates. Perhaps several experiments with a fixed KL-divergence factor would be better to arrive at a correct loss-function balance.
    \item Difficuly to train models for which the KL-divergence term dominates.
    \item A word-dropout of about 75\% seems to work best from qualitative assessments. (These could very well have been cherry-picked)
    \item RNNLMs seem to favour very generic sentences, and are less likely to be diverse.
  \end{itemize}

\bibliographystyle{unsrt}
\bibliography{generating-sentences-from-a-continuous-space}

\end{document}
