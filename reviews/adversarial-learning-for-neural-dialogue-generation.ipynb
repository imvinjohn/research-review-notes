{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Learning for Neural Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors formulate this dialogue model as a reinforcement learning problem. The network used is a Generative Adversarial Network. The discrimnator objective is the same as a Turing test predictor i.e. classifies whether the dialogue response is human or machine-generated. The goal is to improve to improve the generator to the point where the discrimnator has trouble distinguishing between human and machine-generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The generator network is a neural seq2seq model, and the discrimnator is similar to a Turing test evaluator.\n",
    "* The generation task is not formulated as a NMT task. Instead, it tries to maximize the likelihood of a response $y = \\{y_1, y_2 ... y_T\\}$ given a history of previous sentences $x$.\n",
    "* The generator defines the policy by which each word of the output sentence $y$ is generated using a softmax over the space of the vocabulary.\n",
    "* The discrimnator uses a hierarchical neural autoencoder to generate a vector representation of an entire sequence of conversation i.e. $\\{x, y\\}$. This vector representation is then fed into a binary classifier which predicts whether the sentences were human- or machine-generated.\n",
    "* The generator is trained to maximize the expected reward of the generated utterance using the [REINFORCE algorithm](https://link.springer.com/chapter/10.1007/978-1-4615-3618-5_2).\n",
    "* The vanillla REINFORCE model doesn't assign rewards to each generated word, and rather assigns equal reward to all the tokens within a predicted sequence of words.\n",
    "* However, for partially decoded sequences, the discrimnator must also be capable of generating classifications for partial sequences. Two methods are proposed to solve this:\n",
    "    * Using a Monte-Carlo search to decode $N ( = 5)$ top candidate sentences given a partial sequences and using the discrimnator average of the 5 complete sequences to predict the classification for the partial sequence.\n",
    "    * Training the discrimnator to directly also be able to classify partial sequences.\n",
    "* The Monte-Carlo search strategy was found to be more effective.\n",
    "* Teacher forcing is used to essentially short-circuit the distance between the generator and the true sequence.\n",
    "* The generative model is trained using [seq2seq](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural) and an [attention mechanism](https://arxiv.org/abs/1409.0473).The discrimnator is also pretrained using part of the training data and generating sequences by beam-search and sampling.\n",
    "* Intuitively, low accuracy of a reasonably well trained discrimnator would imply that the quality of generated sentences have improved significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The authors report that the responses generated by their system are more interactive, interesting, and\n",
    "non-repetitive. It'd be interesting to see how they quantify this. UPDATE: The source for this claim is human evaluations, which of course, could be subjective.\n",
    "* It's also observed that the system yielded better results when the context i.e. the $x$ preceding utterances were limited to 2.\n",
    "* The hierarchical neural model is the architecture of choice for the discrimnator (evaluator)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv",
   "language": "python",
   "name": ".pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
